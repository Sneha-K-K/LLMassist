weimport matplotlib.pyplot as plt

# Example counts (replace with your actuals)
total_docs = 3119
tcoo_above = 868
coe_above = 920

# Calculate added/removed
added = max(coe_above - tcoo_above, 0)
removed = max(tcoo_above - coe_above, 0)

# Waterfall steps
labels = ["Above Threshold (TCOO)", "Added", "Removed", "Above Threshold (COE)"]
values = [tcoo_above, added, -removed, coe_above]

# Compute cumulative positions for the "flow"
cumulative = [0]  # starting point
for i in range(len(values)-1):
    cumulative.append(cumulative[-1] + values[i])

# Colors: positive (green), negative (red), totals (blue/gray)
colors = []
for v, lbl in zip(values, labels):
    if lbl.startswith("Above Threshold"):
        colors.append("steelblue")
    elif v >= 0:
        colors.append("seagreen")
    else:
        colors.append("indianred")

# Plot
fig, ax = plt.subplots(figsize=(8,5))

for i, (val, base, lbl, color) in enumerate(zip(values, cumulative, labels, colors)):
    ax.bar(i, val, bottom=base, color=color, edgecolor="black")
    ax.text(i, base + val/2, str(val), ha="center", va="center", color="white", fontsize=10, fontweight="bold")

ax.set_xticks(range(len(labels)))
ax.set_xticklabels(labels, rotation=20, ha="right")
ax.set_title("Waterfall of Docs Above Threshold (TCOO → COE)")
ax.set_ylabel("Number of Documents")

plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt

# Example numbers (replace with your real ones)
tcoo_above = 868
coe_above = 920
intersection = 766

swap_in = tcoo_above - intersection    # above only in TCOO
swap_out = coe_above - intersection    # above only in COE

# --- Stacked Bar ---
fig, ax = plt.subplots(figsize=(7,5))

ax.bar("Coverage", intersection, label="Common (Both)", color="steelblue")
ax.bar("Coverage", swap_in, bottom=intersection, label="Swap-in (TCOO only)", color="seagreen")
ax.bar("Coverage", swap_out, bottom=intersection+swap_in, label="Swap-out (COE only)", color="indianred")

ax.set_ylabel("Number of Documents")
ax.set_title("Breakdown of Coverage (>0.5 threshold)")
ax.legend()

# Add annotations
ax.text(0, intersection/2, str(intersection), ha="center", va="center", color="white", fontweight="bold")
ax.text(0, intersection + swap_in/2, str(swap_in), ha="center", va="center", color="white", fontweight="bold")
ax.text(0, intersection + swap_in + swap_out/2, str(swap_out), ha="center", va="center", color="white", fontweight="bold")

plt.tight_layout()
plt.show()


# --- Pie Chart ---
sizes = [intersection, swap_in, swap_out]
labels = ["Common (Both)", "Swap-in (TCOO only)", "Swap-out (COE only)"]
colors = ["steelblue", "seagreen", "indianred"]

plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, wedgeprops={'edgecolor':'black'})
plt.title("Distribution of Docs Above Threshold (TCOO vs COE)")
plt.show()

import pandas as pd
import requests
from bs4 import BeautifulSoup

# Example dataframe
df = pd.DataFrame({
    "doc_id": [1, 1, 1, 2, 2],
    "section_name": ["Intro", "Intro", "Method", "Results", "Results"],
    "text": [
        "Section name: Intro This is part B of intro",
        "Section name: Intro This is part A of intro",
        "Section name: Method Some method description",
        "Section name: Results First chunk",
        "Section name: Results Second chunk"
    ],
    "section_link": [
        "http://example.com/doc1#intro",
        "http://example.com/doc1#intro",
        "http://example.com/doc1#method",
        "http://example.com/doc2#results",
        "http://example.com/doc2#results"
    ]
})


def fetch_section_text(url):
    """Fetch full section text from webpage link"""
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        return soup.get_text(" ", strip=True)
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return ""


def merge_chunks_for_section(df):
    merged_sections = []

    # Group by doc + section
    for (doc, section, link), group in df.groupby(["doc_id", "section_name", "section_link"]):
        webpage_text = fetch_section_text(link)

        # Find each chunk's position in webpage text
        chunk_positions = []
        for text in group["text"]:
            # Remove "Section name: <section>" prefix for position check
            cleaned = text.replace(f"Section name: {section}", "").strip()
            pos = webpage_text.find(cleaned) if cleaned else -1
            chunk_positions.append((pos, text))

        # Sort chunks by their order of appearance in webpage text
        chunk_positions = sorted(chunk_positions, key=lambda x: (x[0] if x[0] != -1 else 999999))

        # Merge text, removing repeated "Section name: <section>" from later chunks
        merged_text = []
        for i, (_, chunk) in enumerate(chunk_positions):
            if i == 0:
                merged_text.append(chunk)  # keep full first chunk
            else:
                merged_text.append(chunk.replace(f"Section name: {section}", "").strip())

        merged_sections.append({
            "doc_id": doc,
            "section_name": section,
            "section_link": link,
            "merged_text": " ".join(merged_text).strip()
        })

    return pd.DataFrame(merged_sections)


merged_df = merge_chunks_for_section(df)
print(merged_df)
import pandas as pd
import re
from difflib import SequenceMatcher

# ── 1. Example: load your dataframes ───────────────────────────────
# Replace these with your actual read statements
# df1 = pd.read_csv("first_doc.csv")   # columns: doc_id, raw_context, ...
# df2 = pd.read_csv("second_doc.csv")  # columns: doc_id, raw_context, ...

# ── 2. Helpers ─────────────────────────────────────────────────────
def normalize(text):
    """Remove special characters, lowercase, squeeze spaces."""
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(text))
    text = re.sub(r'\s+', ' ', text)
    return text.lower().strip()

def strip_first_two_words(text):
    """Remove the first two words before normalization."""
    words = str(text).split()
    return ' '.join(words[2:]) if len(words) > 2 else ''

# ── 3. Pre-process ─────────────────────────────────────────────────
df1['norm_context'] = df1['raw_context'].apply(normalize)
df2['norm_context'] = df2['raw_context'].apply(lambda x: normalize(strip_first_two_words(x)))

# ── 4. Group df1 by document id for faster lookups ─────────────────
df1_grouped = df1.groupby('doc_id')

# ── 5. Match and record results ────────────────────────────────────
matches = []
for idx2, row2 in df2.iterrows():
    doc_id = row2['doc_id']
    context2 = row2['norm_context']
    candidates = df1_grouped.get_group(doc_id) if doc_id in df1_grouped.groups else pd.DataFrame()
    full_match = None
    partial_match = None
    best_ratio = 0.0

    for _, row1 in candidates.iterrows():
        context1 = row1['norm_context']
        # Check full containment of df2 chunk inside df1 chunk
        if context2 and context2 in context1:
            full_match = row1['raw_context']
            break
        # Otherwise compute similarity for potential split overlap
        ratio = SequenceMatcher(None, context2, context1).ratio()
        if ratio > best_ratio:
            best_ratio = ratio
            partial_match = row1['raw_context']

    matches.append({
        'df2_index'     : idx2,
        'doc_id'        : doc_id,
        'raw_context_1' : full_match if full_match else partial_match,
        'full_match'    : bool(full_match),
        'overlap_ratio' : best_ratio
    })

# ── 6. Merge results back into df2 ─────────────────────────────────
match_df = pd.DataFrame(matches).set_index('df2_index')
merged = df2.join(match_df, how='left')

# Flag possible splits
merged['note'] = merged.apply(
    lambda r: 'split/partial' if not r['full_match'] and pd.notna(r['raw_context_1']) else '',
    axis=1
)

# Optionally replace raw_context when a full match exists
merged['raw_context'] = merged.apply(
    lambda r: r['raw_context_1'] if r['full_match'] and pd.notna(r['raw_context_1']) else r['raw_context'],
    axis=1
)

# merged is the final dataframe
print(merged.head())






import re

def strip_doc_sec(text, doc, sec):
    """
    Remove the leading 'doc, sec,' from the raw_context string.
    Matches case-insensitively and ignores extra spaces.
    """
    # Build a regex that tolerates spaces and optional commas
    pattern = rf'^\s*{re.escape(str(doc))}\s*,\s*{re.escape(str(sec))}\s*,\s*'
    return re.sub(pattern, '', str(text), flags=re.IGNORECASE).strip()

df['raw_context'] = df.apply(
    lambda r: strip_doc_sec(r['raw_context'], r['doc_name'], r['sec_name']),
    axis=1
)

import re

df['col1'] = df.apply(
    lambda r: re.sub(rf'^{re.escape(str(r["col4"]))}\s*', '', str(r["col1"])),
    axis=1
)
