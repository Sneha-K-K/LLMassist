import matplotlib.pyplot as plt

# Example counts (replace with your actuals)
total_docs = 3119
tcoo_above = 868
coe_above = 920

# Calculate added/removed
added = max(coe_above - tcoo_above, 0)
removed = max(tcoo_above - coe_above, 0)

# Waterfall steps
labels = ["Above Threshold (TCOO)", "Added", "Removed", "Above Threshold (COE)"]
values = [tcoo_above, added, -removed, coe_above]

# Compute cumulative positions for the "flow"
cumulative = [0]  # starting point
for i in range(len(values)-1):
    cumulative.append(cumulative[-1] + values[i])

# Colors: positive (green), negative (red), totals (blue/gray)
colors = []
for v, lbl in zip(values, labels):
    if lbl.startswith("Above Threshold"):
        colors.append("steelblue")
    elif v >= 0:
        colors.append("seagreen")
    else:
        colors.append("indianred")

# Plot
fig, ax = plt.subplots(figsize=(8,5))

for i, (val, base, lbl, color) in enumerate(zip(values, cumulative, labels, colors)):
    ax.bar(i, val, bottom=base, color=color, edgecolor="black")
    ax.text(i, base + val/2, str(val), ha="center", va="center", color="white", fontsize=10, fontweight="bold")

ax.set_xticks(range(len(labels)))
ax.set_xticklabels(labels, rotation=20, ha="right")
ax.set_title("Waterfall of Docs Above Threshold (TCOO â†’ COE)")
ax.set_ylabel("Number of Documents")

plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt

# Example numbers (replace with your real ones)
tcoo_above = 868
coe_above = 920
intersection = 766

swap_in = tcoo_above - intersection    # above only in TCOO
swap_out = coe_above - intersection    # above only in COE

# --- Stacked Bar ---
fig, ax = plt.subplots(figsize=(7,5))

ax.bar("Coverage", intersection, label="Common (Both)", color="steelblue")
ax.bar("Coverage", swap_in, bottom=intersection, label="Swap-in (TCOO only)", color="seagreen")
ax.bar("Coverage", swap_out, bottom=intersection+swap_in, label="Swap-out (COE only)", color="indianred")

ax.set_ylabel("Number of Documents")
ax.set_title("Breakdown of Coverage (>0.5 threshold)")
ax.legend()

# Add annotations
ax.text(0, intersection/2, str(intersection), ha="center", va="center", color="white", fontweight="bold")
ax.text(0, intersection + swap_in/2, str(swap_in), ha="center", va="center", color="white", fontweight="bold")
ax.text(0, intersection + swap_in + swap_out/2, str(swap_out), ha="center", va="center", color="white", fontweight="bold")

plt.tight_layout()
plt.show()


# --- Pie Chart ---
sizes = [intersection, swap_in, swap_out]
labels = ["Common (Both)", "Swap-in (TCOO only)", "Swap-out (COE only)"]
colors = ["steelblue", "seagreen", "indianred"]

plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, wedgeprops={'edgecolor':'black'})
plt.title("Distribution of Docs Above Threshold (TCOO vs COE)")
plt.show()

import pandas as pd
import requests
from bs4 import BeautifulSoup

# Example dataframe
df = pd.DataFrame({
    "doc_id": [1, 1, 1, 2, 2],
    "section_name": ["Intro", "Intro", "Method", "Results", "Results"],
    "text": [
        "Section name: Intro This is part B of intro",
        "Section name: Intro This is part A of intro",
        "Section name: Method Some method description",
        "Section name: Results First chunk",
        "Section name: Results Second chunk"
    ],
    "section_link": [
        "http://example.com/doc1#intro",
        "http://example.com/doc1#intro",
        "http://example.com/doc1#method",
        "http://example.com/doc2#results",
        "http://example.com/doc2#results"
    ]
})


def fetch_section_text(url):
    """Fetch full section text from webpage link"""
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        return soup.get_text(" ", strip=True)
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return ""


def merge_chunks_for_section(df):
    merged_sections = []

    # Group by doc + section
    for (doc, section, link), group in df.groupby(["doc_id", "section_name", "section_link"]):
        webpage_text = fetch_section_text(link)

        # Find each chunk's position in webpage text
        chunk_positions = []
        for text in group["text"]:
            # Remove "Section name: <section>" prefix for position check
            cleaned = text.replace(f"Section name: {section}", "").strip()
            pos = webpage_text.find(cleaned) if cleaned else -1
            chunk_positions.append((pos, text))

        # Sort chunks by their order of appearance in webpage text
        chunk_positions = sorted(chunk_positions, key=lambda x: (x[0] if x[0] != -1 else 999999))

        # Merge text, removing repeated "Section name: <section>" from later chunks
        merged_text = []
        for i, (_, chunk) in enumerate(chunk_positions):
            if i == 0:
                merged_text.append(chunk)  # keep full first chunk
            else:
                merged_text.append(chunk.replace(f"Section name: {section}", "").strip())

        merged_sections.append({
            "doc_id": doc,
            "section_name": section,
            "section_link": link,
            "merged_text": " ".join(merged_text).strip()
        })

    return pd.DataFrame(merged_sections)


merged_df = merge_chunks_for_section(df)
print(merged_df)
