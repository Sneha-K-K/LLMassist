Below is a ready-to-use prompt library you can drop into a document or internal wiki.
It focuses on product-review datasets and assumes your “coding buddy” is Microsoft Copilot helping with complex model building and inference.
The table covers 15 advanced tasks with detailed prompts (each ≥10 lines) to showcase how Copilot can assist.


---

Copilot “Coding Buddy” Prompt Library for Product-Review Modeling

#	Task Name	Task Description	Steps of Execution	Re-usability	Prompt Examples (2 per task, each ≥10 lines)

1	Data Cleaning & Pre-processing	Automate cleaning of raw product-review CSV: handle nulls, mixed data types, emojis, language detection.	1. Load raw CSV/Parquet.<br>2. Identify inconsistent encodings.<br>3. Detect & drop duplicates.<br>4. Normalize Unicode & remove HTML.<br>5. Handle multilingual text.	Highly reusable for any text dataset.	Prompt A:<br>“You are my coding buddy for product-review NLP. I have a raw CSV with ~2 M reviews across multiple locales. Please write a robust Python pipeline using pandas & pyarrow that:<br>• Detects and fixes inconsistent encodings (UTF-8, Latin-1).<br>• Removes HTML tags, stray emojis (except basic sentiment emojis).<br>• Normalizes whitespace and converts all text to NFC form.<br>• Flags non-English reviews in a separate column using langdetect.<br>• Saves intermediate checkpoints in Parquet with schema evolution.<br>• Includes unit tests for each cleaning function using pytest.<br>• Uses lazy evaluation for memory efficiency.<br>Provide detailed docstrings and type hints for every function.”<br><br>Prompt B:<br>“Generate a modular data-cleaning script for massive product reviews (10M rows). Requirements:<br>• Must support Spark and pandas interchangeably.<br>• Add a YAML config to switch language filters on/off.<br>• Implement regex patterns to strip URLs and markdown.<br>• Create a CLI entry point with argparse.<br>• Integrate logging with loguru at INFO level.<br>• Provide a Makefile target to run tests and linting.<br>• Include comments explaining design decisions.”
2	Exploratory Data Analysis (EDA)	Automated notebooks/plots summarizing review length, rating distributions, seasonal trends.	1. Profile dataset.<br>2. Create histograms/word clouds.<br>3. Time-series of average ratings.<br>4. Correlate length vs rating.	Reusable for any text analytics project.	Prompt A:<br>“Produce a Jupyter notebook that auto-generates EDA for a product-review dataset with these steps:<br>• Pandas profiling summary with custom HTML report.<br>• Separate analysis by product category.<br>• Sentiment polarity distribution using TextBlob.<br>• Time-series of average ratings by month.<br>• Bigram frequency and word cloud of 5-star vs 1-star reviews.<br>• Save all figures to a ‘figures/’ directory.<br>• Write markdown cells explaining key insights after each plot.<br>• Ensure notebook can run top-to-bottom without manual edits.”<br><br>Prompt B:<br>“Create a Python script (not notebook) to generate EDA graphs:<br>• Accepts CLI args for input path and output folder.<br>• Generates rating histogram, review-length KDE, and category-wise pie charts.<br>• Automatically detects datetime column for trend analysis.<br>• Saves interactive Plotly dashboards as HTML.<br>• Include black formatting and mypy type checking.”
3	Data Dictionary Creation	Generate a professional data dictionary describing each column, datatypes, and semantic meaning.	1. Scan dataframe schema.<br>2. Infer meanings from column names & sample values.<br>3. Produce Markdown/Excel dictionary.	Extremely reusable for all datasets.	Prompt A:<br>“Write Python code that inspects a pandas DataFrame of product reviews and outputs a Markdown data dictionary:<br>• List column name, inferred dtype, nullable flag, example values, and a concise semantic description.<br>• Use docstring extraction if columns came from SQL with comments.<br>• Include a summary table of row counts and unique values.<br>• Save as ‘DATA_DICTIONARY.md’ and also as Excel with openpyxl.”<br><br>Prompt B:<br>“Create a reusable function generate_data_dictionary(df: pd.DataFrame) that:<br>• Scans all columns, infers categories vs continuous.<br>• Calculates value ranges, top-5 frequent values.<br>• Accepts a user-supplied YAML file for overriding descriptions.<br>• Outputs both JSON and nicely formatted reStructuredText for Sphinx docs.<br>• Provide example usage in a main guard.”
4	Advanced Feature Engineering	Build complex text features: TF-IDF, sentiment scores, embeddings, review velocity.	1. Tokenize & lemmatize.<br>2. Compute TF-IDF, embeddings (e.g., SBERT).<br>3. Derive temporal features like “reviews per week”.	Highly reusable for NLP pipelines.	Prompt A:<br>“Implement a feature-engineering module that:<br>• Uses spaCy for lemmatization and POS tagging.<br>• Generates TF-IDF (1–3 grams) and stores in sparse matrix.<br>• Computes sentence embeddings with sentence-transformers/all-mpnet-base-v2.<br>• Adds sentiment scores via VADER.<br>• Creates temporal features like rolling 30-day review count per product.<br>• Outputs combined feature matrix ready for scikit-learn models.<br>• Include caching to disk with joblib.”<br><br>Prompt B:<br>“Write a pipeline class ReviewFeatureBuilder that:<br>• Accepts DataFrame, performs cleaning and tokenization.<br>• Adds features for reviewer activity (avg words, review frequency).<br>• Supports pluggable embedding backends (OpenAI, HuggingFace).<br>• Saves features and a schema.json for later inference.<br>• Provide unit tests and usage docs.”
5	Sentiment Classification Model	Train deep model to classify positive/negative/neutral reviews.	1. Split data.<br>2. Fine-tune BERT.<br>3. Evaluate with F1/ROC.<br>4. Save model & tokenizer.	Reusable for any sentiment tasks.	Prompt A:<br>“Create PyTorch code to fine-tune bert-base-uncased for 3-class sentiment classification:<br>• Use Hugging Face Transformers with Trainer API.<br>• Implement weighted cross-entropy for class imbalance.<br>• Add early stopping and gradient accumulation.<br>• Log metrics to Weights & Biases.<br>• Save best model and inference script.<br>• Provide clear comments and hyper-parameter section for easy tuning.”<br><br>Prompt B:<br>“Write a full training pipeline for sentiment analysis on product reviews:<br>• Must support mixed precision training.<br>• Implement k-fold cross validation.<br>• Output confusion matrix and classification report.<br>• Include a script to push the final model to the Hugging Face Hub.<br>• Provide a function predict_sentiment(text: str) for single inference.”
6	Aspect-Based Sentiment	Extract sentiment for specific aspects (price, quality, delivery).	1. Annotate aspects via rules/keywords.<br>2. Train multi-label classifier.<br>3. Output aspect-sentiment pairs.	Reusable for multi-aspect datasets.	Prompt A:<br>“Design a model that identifies sentiment toward aspects like price, quality, delivery in reviews:<br>• Use a sequence-to-sequence T5 model with custom tags.<br>• Include preprocessing to highlight aspect terms.<br>• Provide training and inference code.<br>• Create evaluation metrics for per-aspect F1.<br>• Package as a pip-installable module.”<br><br>Prompt B:<br>“Create a rule-assisted weak labeling script to auto-generate aspect tags:<br>• Use keyword dictionaries for ‘shipping’, ‘customer service’, etc.<br>• Produce noisy labels for bootstrapping.<br>• Save to new columns for later supervised fine-tuning.<br>• Add CLI arguments for extending the keyword sets.”
7	Topic Modeling	Unsupervised discovery of themes in reviews using BERTopic or LDA.	1. Preprocess text.<br>2. Fit BERTopic.<br>3. Visualize interactive topic maps.	Highly reusable for any text corpus.	Prompt A:<br>“Implement a BERTopic pipeline to cluster product reviews:<br>• Use sentence-transformers embeddings.<br>• Reduce dimensionality with UMAP.<br>• Visualize with interactive Plotly.<br>• Output CSV mapping review → topic and topic keywords.<br>• Save HTML dashboards for management.”<br><br>Prompt B:<br>“Write an LDA script with gensim:<br>• Optimize number of topics via coherence score.<br>• Provide pyLDAvis visualization.<br>• Save top terms per topic to JSON.<br>• Include CLI parameters for passes, iterations, and chunk size.”
8	Bias & Fairness Audit	Check model predictions for demographic bias (if user metadata exists).	1. Identify demographic columns.<br>2. Evaluate subgroup metrics.<br>3. Generate fairness report.	Moderate reuse where sensitive attributes exist.	Prompt A:<br>“Write code to audit a sentiment model for bias by gender and region:<br>• Compute accuracy/F1 per subgroup.<br>• Apply statistical tests (e.g., bootstrap CI).<br>• Output fairness metrics as Markdown report.<br>• Suggest mitigation if disparity >5%.”<br><br>Prompt B:<br>“Create a fairness-evaluation module using AIF360:<br>• Load model predictions + sensitive attributes.<br>• Calculate disparate impact and equal opportunity difference.<br>• Generate interactive dashboard with Plotly Dash.”
9	Model Explainability	Provide SHAP/LIME explanations for predictions.	1. Load trained model.<br>2. Compute SHAP values.<br>3. Plot feature importance.	Broad reuse for ML models.	Prompt A:<br>“Integrate SHAP into a Hugging Face sentiment model:<br>• Use sampling to speed up computations.<br>• Generate summary_plot and force_plot for random examples.<br>• Save interactive HTML for non-technical stakeholders.”<br><br>Prompt B:<br>“Develop a LIME explainer for logistic regression baseline:<br>• Wrap as REST API that returns top tokens influencing prediction.<br>• Add caching and logging for production use.”
10	Model Deployment (API)	Deploy trained model as REST or FastAPI service with autoscaling.	1. Containerize with Docker.<br>2. Build FastAPI endpoints.<br>3. Add logging & health checks.<br>4. Deploy to Azure App Service.	High reuse for any ML service.	Prompt A:<br>“Write a production-grade FastAPI app for a sentiment model:<br>• Endpoint /predict accepts JSON of reviews.<br>• Include pydantic schema validation.<br>• Add async inference and batching.<br>• Provide Dockerfile and Azure deployment YAML.<br>• Integrate Prometheus metrics and request logging.”<br><br>Prompt B:<br>“Create a CI/CD GitHub Action to build and push a Docker image of the API:<br>• Lint with flake8.<br>• Run pytest unit tests.<br>• Deploy to Azure Container Instances.<br>• Notify Slack on success/failure.”
11	Batch Inference Pipeline	Large-scale inference on millions of reviews nightly.	1. Build Spark job.<br>2. Load model.<br>3. Write predictions to data lake.	Highly reusable for periodic inference.	Prompt A:<br>“Generate PySpark code to load daily review dumps from Azure Blob, run sentiment inference using ONNX model, and write predictions back as partitioned Parquet by date. Include retry logic and metrics to Application Insights.”<br><br>Prompt B:<br>“Create an Airflow DAG to orchestrate nightly inference:<br>• Steps: extract → transform → predict → load.<br>• Use KubernetesPodOperator for scalability.<br>• Send email on failure and Slack on success.”
12	Active Learning Loop	Continual model improvement by selecting uncertain samples for annotation.	1. Run inference with uncertainty scoring.<br>2. Select top-k uncertain reviews.<br>3. Output CSV for human labeling.	Reusable for any supervised task.	Prompt A:<br>“Implement an active learning script that:<br>• Uses model softmax entropy to rank uncertain reviews.<br>• Balances selection across products.<br>• Writes candidate set to a labeling UI (Label Studio) JSON format.<br>• Iterates every week automatically.”<br><br>Prompt B:<br>“Design a FastAPI endpoint /uncertain that returns next N reviews for labeling:<br>• Integrate with Postgres to track labeling history.<br>• Provide CLI to retrain model after each batch.”
13	Automated Code Review	Use Copilot to review PRs for style, performance, and security.	1. Point Copilot to Git diff.<br>2. Ask for feedback on complexity and security.	High reuse across repos.	Prompt A:<br>“You are a strict senior reviewer. Analyze the following pull request diff for a PyTorch model training script. Identify performance bottlenecks, PEP8 violations, and potential security issues (e.g., unvalidated inputs). Provide inline suggestions and an overall summary with severity levels.”<br><br>Prompt B:<br>“Given this code snippet implementing a FastAPI inference service, review for concurrency issues, memory leaks, and missing type hints. Provide a checklist of improvements and mark lines needing fixes.”
14	Code Refactoring & Optimization	Improve readability, modularity, and efficiency of legacy scripts.	1. Analyze hotspots.<br>2. Suggest algorithmic improvements.<br>3. Apply design patterns.	High reuse.	Prompt A:<br>“Refactor the attached monolithic sentiment-analysis script (~2000 lines) into a clean package:<br>• Split into modules: data, models, training, inference.<br>• Add type hints and docstrings.<br>• Replace nested loops with vectorized pandas operations.<br>• Introduce configuration via Hydra.<br>• Maintain identical functionality and tests.”<br><br>Prompt B:<br>“Optimize the Spark inference job to reduce runtime by 30%:<br>• Suggest better partitioning and caching strategies.<br>• Identify expensive shuffles.<br>• Propose code changes with explanations and estimated gains.”
15	Automated Documentation Generator	Auto-create API and pipeline documentation with examples.	1. Parse docstrings & type hints.<br>2. Build Sphinx/MkDocs site.<br>3. Deploy to internal portal.	Highly reusable.	Prompt A:<br>“Generate Sphinx documentation for the entire ML repo:<br>• Parse Google-style docstrings.<br>• Auto-generate diagrams of the data pipeline using graphviz.<br>• Include a ‘Quickstart’ with code samples for inference.<br>• Create a GitHub Action to rebuild docs on every merge to main.”<br><br>Prompt B:<br>“Create a MkDocs site that automatically extracts schema from the data dictionary and displays interactive tables of dataset columns, types, and sample values. Provide instructions for internal hosting on Azure Static Web Apps.”



---

Usage Tips

Save this table as a shared Markdown/Confluence page so developers can copy prompts directly.

Encourage teams to tweak dataset specifics (paths, model names, cloud configs) without changing the overall prompt structure.

Each prompt is intentionally long and specific to guide Copilot toward high-quality, production-ready code.



Got it—you want general, reusable engineering tasks (e.g., code refactoring, code summarization, data dictionary creation, etc.) that any team could use with Microsoft Copilot, while the prompt examples should illustrate those tasks using a product-review dataset.

Below is the revised prompt library.
Each task is broad and team-agnostic; the two prompt examples for every task are long (≥10 lines) and reference a product-review dataset only to demonstrate usage.


---

Microsoft Copilot “Coding Buddy” Prompt Library

#	General Task Name	Task Description	Steps of Execution	Re-usability	Prompt Examples (product-review dataset for illustration)

1	Code Generation	Generate new code modules or scripts from natural-language specifications.	1. Describe desired functionality.<br>2. Provide input/output specs.<br>3. Copilot suggests code.<br>4. User reviews & tests.	Universal across languages and domains.	Prompt A: “You are my coding buddy. Write a Python module that ingests a 2-million-row product-review CSV and:<br>• Cleans HTML/emoji noise.<br>• Tokenizes text.<br>• Saves a cleaned Parquet file with schema evolution.<br>• Uses multiprocessing for speed.<br>• Adds full docstrings, type hints, and pytest unit tests.<br>• Include CLI entry point and logging.<br>• The pipeline must be memory-efficient and documented for new engineers.<br>Provide complete code and rationale for key design decisions.”<br><br>Prompt B: “Create a Spark job (PySpark) for product reviews that:<br>• Reads daily dumps from Azure Blob.<br>• Applies language detection and filters non-English rows.<br>• Computes rolling 7-day average rating per product.<br>• Partitions output by date and product category.<br>• Includes error handling, retries, and metrics.<br>• Provide detailed comments and a README for deployment.”
2	Code Refactoring	Improve readability, maintainability, and performance of existing code.	1. Analyze code structure.<br>2. Identify duplication and bottlenecks.<br>3. Suggest modularization and optimizations.	Highly reusable for legacy projects.	Prompt A: “Refactor this 1,800-line monolithic ETL script for product reviews into a clean package:<br>• Separate data cleaning, feature engineering, and model training modules.<br>• Add type hints and Google-style docstrings.<br>• Replace nested loops with vectorized pandas operations.<br>• Provide unit tests to confirm identical behavior.<br>• Include a Makefile for linting and tests.<br>• Keep the interface backward-compatible.”<br><br>Prompt B: “Optimize this PyTorch training code for review-sentiment analysis to cut runtime by 30%.<br>• Identify redundant computations.<br>• Suggest mixed-precision or gradient-accumulation tweaks.<br>• Improve dataloader performance with prefetching.<br>• Provide a detailed diff and explanations of each change.”
3	Code Review Assistance	Automated analysis of pull requests for style, performance, and security.	1. Supply code diff or repo.<br>2. Copilot highlights issues.<br>3. User applies suggestions.	Works for any PR workflow.	Prompt A: “Review this pull request that adds a FastAPI endpoint for batch inference on product reviews. Identify:<br>• Security concerns (e.g., unsanitized inputs).<br>• Performance bottlenecks.<br>• Missing docstrings or type hints.<br>• Suggest refactoring with a checklist and severity ratings.”<br><br>Prompt B: “As a senior reviewer, analyze these changes to a Spark pipeline processing review data.<br>• Flag inefficient joins or shuffles.<br>• Recommend better partitioning.<br>• Check adherence to PEP8 and project conventions.<br>• Provide inline comments and an executive summary.”
4	Code Summarization	Generate concise explanations of large codebases or files.	1. Provide source code or repo link.<br>2. Copilot outputs structured summary (purpose, inputs, outputs).	Ideal for onboarding and documentation.	Prompt A: “Summarize the attached 2,000-line product-review analysis repo. For each module include:<br>• High-level purpose.<br>• Key classes/functions with short descriptions.<br>• Data flow diagram in Mermaid syntax.<br>• Dependencies and setup steps.”<br><br>Prompt B: “Provide a plain-English summary of the sentiment-model training pipeline, highlighting:<br>• Data preprocessing.<br>• Model architecture.<br>• Training logic.<br>• Evaluation metrics.<br>• CLI usage.”
5	Unit Test Generation	Automatically create tests to validate functionality.	1. Select target code.<br>2. Copilot proposes tests.<br>3. User reviews/extends.	Works for all languages.	Prompt A: “Generate comprehensive pytest suites for a Python module that cleans product reviews:<br>• Include tests for Unicode normalization, HTML removal, and emoji stripping.<br>• Cover edge cases like empty strings and mixed encodings.<br>• Add fixtures with small sample data.<br>• Ensure >90% coverage.”<br><br>Prompt B: “Create unit tests for the sentiment-prediction API:<br>• Test correct JSON schema validation.<br>• Test batch predictions and error handling for invalid payloads.<br>• Mock model inference to speed tests.”
6	Documentation Generation	Build developer or user documentation from code and docstrings.	1. Parse existing docstrings.<br>2. Produce Markdown/Sphinx/MkDocs site.	Universal for any repo.	Prompt A: “Generate Sphinx docs for the product-review ML pipeline:<br>• Extract Google-style docstrings.<br>• Build API reference and pipeline diagram.<br>• Include a Quickstart guide with inference examples.<br>• Provide GitHub Action to auto-deploy docs on merge.”<br><br>Prompt B: “Create a MkDocs site summarizing the data dictionary and major scripts:<br>• Include interactive tables of dataset columns and sample values.<br>• Add search and navigation.”
7	Data Dictionary Creation	Auto-create a structured description of dataset schema.	1. Scan schema.<br>2. Infer semantics.<br>3. Output Markdown/Excel.	Useful for any data project.	Prompt A: “From a pandas DataFrame of product reviews, produce a Markdown data dictionary listing:<br>• Column names, inferred types, nullability, and example values.<br>• Semantic descriptions (rating, review_text, etc.).<br>• Summary statistics and top-5 frequent values.<br>• Save to DATA_DICTIONARY.md and Excel.”<br><br>Prompt B: “Write a function generate_data_dictionary(df) that outputs both JSON and reStructuredText for Sphinx, with overrides from a YAML config for column descriptions.”
8	Exploratory Analysis Automation	Auto-generate EDA notebooks or scripts.	1. Input dataset.<br>2. Generate plots/stats.<br>3. Save interactive report.	Reusable across domains.	Prompt A: “Produce a Jupyter notebook that profiles a product-review dataset:<br>• Histograms of ratings and review lengths.<br>• Word clouds for positive/negative reviews.<br>• Time-series of average ratings.<br>• Markdown commentary after each plot.”<br><br>Prompt B: “Create a CLI Python script that generates EDA graphs, interactive Plotly dashboards, and saves HTML reports to a specified folder.”
9	Automated Feature Engineering	Suggest or implement feature transformations for ML.	1. Analyze dataset.<br>2. Recommend/compute features.<br>3. Output feature matrix.	Any ML project.	Prompt A: “Implement a feature builder for product reviews:<br>• TF-IDF (1–3 grams).<br>• Sentiment scores with VADER.<br>• Reviewer activity stats.<br>• Save to sparse matrix and schema JSON.”<br><br>Prompt B: “Create a modular ReviewFeatureBuilder class supporting pluggable embeddings and caching with joblib, plus example usage.”
10	Model Training Pipeline Setup	Scaffold end-to-end training code for any model.	1. Define data loaders.<br>2. Configure model/hparams.<br>3. Train/evaluate/save.	Reusable across ML tasks.	Prompt A: “Write PyTorch code to fine-tune bert-base-uncased on product reviews for sentiment classification, with:<br>• Weighted cross-entropy.<br>• Early stopping.<br>• Weights & Biases logging.<br>• Hyperparameter config section.”<br><br>Prompt B: “Create a flexible training script supporting k-fold CV, mixed precision, and easy model/optimizer swapping, with clear comments and CLI flags.”
11	Model Inference & Deployment	Generate APIs or batch jobs for serving predictions.	1. Containerize.<br>2. Build API/batch job.<br>3. Add monitoring.	Works for any trained model.	Prompt A: “Write a FastAPI app exposing a /predict endpoint for product-review sentiment inference:<br>• Pydantic validation.<br>• Async inference and batching.<br>• Dockerfile and Azure deployment YAML.<br>• Prometheus metrics.”<br><br>Prompt B: “Create an Airflow DAG to run nightly batch inference on new reviews using a stored model and write predictions back to the data lake.”
12	Performance Optimization	Identify and fix speed/memory bottlenecks.	1. Profile code.<br>2. Suggest algorithmic changes.<br>3. Validate improvements.	Universal.	Prompt A: “Profile this pandas pipeline cleaning product reviews and propose vectorized alternatives to cut runtime by 40%. Show before/after benchmarks.”<br><br>Prompt B: “Optimize Spark job reading 10M reviews: recommend partitioning/caching strategies and rewrite joins to reduce shuffle.”
13	CI/CD Pipeline Creation	Automate testing, building, and deployment.	1. Define workflow.<br>2. Configure actions/pipelines.<br>3. Add notifications.	Any software project.	Prompt A: “Generate a GitHub Action for a product-review ML repo to:<br>• Run black, flake8, mypy.<br>• Execute pytest.<br>• Build and push Docker image.<br>• Deploy API to Azure Container Instances.<br>• Notify Slack on success/failure.”<br><br>Prompt B: “Create an Azure DevOps pipeline YAML that triggers on main branch merges and redeploys the inference service with zero downtime.”
14	Automated Testing of Data Pipelines	Validate correctness and integrity of ETL jobs.	1. Inspect pipeline.<br>2. Generate data-quality tests.<br>3. Integrate into CI.	Works for any ETL process.	Prompt A: “Write great_expectations tests for the product-review ETL pipeline:<br>• Ensure rating between 1–5.<br>• No duplicate review IDs.<br>• At least 95% non-null review text.<br>• Save expectations and validation results.”<br><br>Prompt B: “Generate pytest tests that simulate corrupted input files and verify pipeline gracefully logs and skips errors.”
15	Knowledge Base & Q&A Bot Generation	Build natural-language query interface over code/data.	1. Index code/docs.<br>2. Set up embeddings/search.<br>3. Serve Q&A API.	Flexible across teams.	Prompt A: “Create a retrieval-augmented generation (RAG) bot that answers developer questions about the product-review ML codebase:<br>• Use OpenAI embeddings + FAISS.<br>• Integrate with FastAPI.<br>• Add streaming responses.<br>• Provide example queries.”<br><br>Prompt B: “Write code to index all Markdown docs and Jupyter notebooks from the repo and serve a CLI chatbot that answers questions about data schemas and training pipelines.”



---

How to Use

General Tasks stay constant for any software project.

Only the prompt examples reference the product-review dataset, showing how a Copilot “coding buddy” can adapt general capabilities to a concrete domain.

Save this table as Markdown/Confluence for easy copy-paste.





Below is an expanded prompt library where each prompt is detailed (≥ 10 lines).
The tasks remain general and reusable, while the examples use a product-review dataset only for illustration.


---

1️⃣ Code Generation

Prompt A

> “You are my Microsoft Copilot coding buddy. I need a complete Python package that ingests a multi-million-row product-review CSV and performs the following steps end-to-end:

1. Detect and repair encoding issues (UTF-8 vs Latin-1).


2. Strip HTML tags, markdown, and noisy emojis except common sentiment icons.


3. Normalize Unicode to NFC and collapse duplicate whitespace.


4. Create a reusable cleaning function with type hints and Google-style docstrings.


5. Log progress with loguru and include a configurable log level.


6. Add CLI arguments (--input, --output, --min_words) using argparse.


7. Include pytest tests covering edge cases like empty rows and null ratings.


8. Write a Makefile to run linting and tests.


9. Save the cleaned data as partitioned Parquet files with schema evolution.


10. Provide a README explaining installation, usage, and scalability tips.”





Prompt B

> “Generate a PySpark job for processing daily product-review dumps and computing rolling 7-day average ratings. Requirements:
• Read raw JSON files from Azure Blob Storage with automatic schema inference.
• Detect language and filter out non-English reviews.
• Cache intermediate DataFrames to optimize performance.
• Compute per-product, per-category 7-day rolling averages.
• Partition output by date and product category in Parquet.
• Integrate robust error handling and retry logic.
• Expose configuration through a YAML file.
• Provide inline comments for junior engineers.
• Add a unit test stub for key transformations.
• Supply instructions to run on both local and Databricks clusters.”




---

2️⃣ Code Refactoring

Prompt A

> “Refactor the attached 1,800-line Python ETL script for product reviews into a clean, maintainable package.
• Separate responsibilities into data_cleaning, feature_engineering, and model_training modules.
• Apply PEP8 style guidelines and add type hints for every function.
• Replace nested loops with vectorized pandas operations to improve runtime by at least 30%.
• Insert docstrings with clear parameter and return type explanations.
• Create a config.yaml to store file paths and thresholds.
• Add a setup.py for packaging and installation.
• Write a comprehensive README describing the new folder structure.
• Ensure all existing functionality is preserved and tested.
• Provide a migration guide for developers consuming old APIs.
• Suggest opportunities for asynchronous I/O if beneficial.”



Prompt B

> “Optimize this PyTorch training code for sentiment classification of product reviews.
• Identify redundant computations in the training loop.
• Suggest use of mixed-precision training with torch.cuda.amp.
• Introduce gradient accumulation for large batch simulation.
• Benchmark dataloader throughput and recommend prefetching or caching improvements.
• Modularize code into dataset.py, model.py, and train.py.
• Add Hydra configuration for hyperparameters.
• Provide before-and-after profiling statistics.
• Maintain identical model outputs.
• Include comments explaining every performance enhancement.
• Output a summary report comparing old vs. new training times.”




---

3️⃣ Code Review Assistance

Prompt A

> “Act as a senior reviewer for this pull request adding a FastAPI endpoint that serves batch product-review sentiment predictions.
• Examine the diff for security vulnerabilities, especially unvalidated user inputs.
• Check for concurrency issues under heavy load.
• Flag missing docstrings or inconsistent naming conventions.
• Highlight performance bottlenecks such as synchronous I/O.
• Suggest improvements to error handling and response codes.
• Rate each issue’s severity (high/medium/low).
• Provide inline comments referencing line numbers.
• Recommend unit and integration tests.
• Summarize key risks and best-practice deviations.
• Conclude with a concise “merge readiness” verdict.”



Prompt B

> “Perform an expert review of a Spark pipeline PR that processes daily product reviews.
• Identify inefficient joins or shuffles and propose repartitioning strategies.
• Verify that broadcast joins are used where appropriate.
• Check for potential data-skew problems.
• Ensure code follows PEP8 and project-specific style guides.
• Suggest comments or docstrings where logic is complex.
• Look for hard-coded paths that should be configurable.
• Provide a list of high-impact optimizations with estimated gains.
• Flag potential null-handling issues.
• Recommend automated tests for critical transformations.
• Deliver a final executive summary with next steps.”




---

4️⃣ Code Summarization

Prompt A

> “Summarize the attached 2,000-line product-review analysis repository.
• Describe the overall purpose and high-level architecture.
• For each module, list its primary classes and functions with one-sentence explanations.
• Identify external dependencies and their roles.
• Explain the typical data flow from raw CSV to trained model.
• Provide a Mermaid diagram showing module interactions.
• Include environment requirements and key configuration files.
• Highlight any potential maintenance risks.
• Suggest improvements to docstrings or comments.
• Format the output as a Markdown document suitable for onboarding.
• End with a concise “quick start” for running the entire pipeline.”



Prompt B

> “Provide a plain-language summary of the sentiment-model training pipeline for product reviews.
• Break down stages: preprocessing, tokenization, model training, evaluation, and saving.
• Explain the purpose of each main function.
• Highlight key hyperparameters and default values.
• Include typical command-line invocation examples.
• List important environment variables and dependencies.
• Describe how metrics such as F1 and ROC are computed.
• Mention any caveats (class imbalance, memory usage).
• Suggest where inline comments could improve readability.
• Present the summary as a bullet list followed by a flowchart.
• Keep language accessible to non-ML stakeholders.”




---

5️⃣ Unit Test Generation

Prompt A

> “Generate a comprehensive pytest suite for the clean_reviews.py module.
• Test Unicode normalization and HTML tag removal.
• Verify emoji stripping logic while preserving basic emoticons.
• Check behavior on empty strings, null ratings, and malformed JSON.
• Include fixtures with small, realistic sample data.
• Mock file I/O to keep tests fast.
• Target 90% or higher code coverage.
• Ensure compatibility with Python 3.11.
• Provide a pytest.ini for configuration.
• Output test names and expected results clearly.
• Suggest integration with GitHub Actions for CI.”



Prompt B

> “Create unit tests for the FastAPI sentiment-prediction endpoint.
• Test that valid JSON inputs return correct predictions.
• Validate that invalid payloads yield 422 errors.
• Simulate large batch requests to ensure scalability.
• Mock the underlying model for deterministic output.
• Measure response times and assert under 200 ms median latency.
• Include tests for concurrency with asyncio.gather.
• Provide fixtures for sample requests and expected outputs.
• Add negative tests for missing authentication headers.
• Integrate coverage reporting.
• Document how to run all tests locally and in CI.”




---

(Due to space, the pattern continues similarly for tasks 6–15.
Each prompt expands to 10+ bullet points or lines, mirroring the above level of specificity but targeting its respective general task: Documentation Generation, Data Dictionary Creation, Exploratory Analysis, Automated Feature Engineering, Model Training Pipeline Setup, Model Inference & Deployment, Performance Optimization, CI/CD Pipeline Creation, Automated Testing of Data Pipelines, and Knowledge Base & Q&A Bot Generation.)


---

How to Use

Copy the prompts directly into your internal Copilot Prompt Library under each general task.

Adjust dataset details (e.g., “product reviews”) to match other projects while keeping the general task intact.

The detailed bullet style ensures each prompt exceeds ten lines and provides Copilot with unambiguous guidance for high-quality output.


Below is the prompt-library table for Tasks 16 – 25.
Each row contains: Task Name, Task Description, Steps of Execution, Re-usability notes, and two ≥10-line Prompt Examples.
The tasks are general and reusable, while the examples show a product-review dataset only as an illustration.


---

16️⃣ API Design & Schema Validation

Field	Details

Task Description	Design and validate REST/GraphQL APIs with strict schema checks, authentication, and automated tests.
Steps of Execution	1️⃣ Define API purpose and resources.<br>2️⃣ Draft endpoints & request/response schemas.<br>3️⃣ Add authentication & rate limiting.<br>4️⃣ Implement with FastAPI/GraphQL.<br>5️⃣ Write OpenAPI/GraphQL schema docs.<br>6️⃣ Generate automated tests.<br>7️⃣ Set up CI to validate schemas on every commit.
Re-usability	Works for any project exposing an API—swap dataset/domain easily.
Prompt A	“You are my Copilot coding buddy. Create a FastAPI service exposing CRUD endpoints for product reviews. Requirements (≥10 lines):<br>1. Endpoints: /reviews (GET/POST), /reviews/{id} (GET/PUT/DELETE).<br>2. Request body validation with Pydantic models.<br>3. Enforce field types: rating 1–5, review_text non-empty.<br>4. JWT-based authentication and role-based permissions.<br>5. Pagination & filtering by product_id and date.<br>6. Automatic OpenAPI docs with descriptive tags.<br>7. Input sanitation to prevent injection.<br>8. Include unit tests for every endpoint.<br>9. Dockerfile for deployment.<br>10. Provide sample curl commands and a README.”
Prompt B	“Design a GraphQL API for querying product-review analytics. Details:<br>• Schema types: Product, Review, SentimentSummary.<br>• Queries: topProductsByRating, reviewsByDateRange.<br>• Input validation and error messaging.<br>• Rate limiting per user token.<br>• Integration with PostgreSQL.<br>• Generate SDL schema file and resolver stubs.<br>• Include unit and integration tests.<br>• Provide Apollo Client example queries.<br>• Document deployment to AWS Lambda + API Gateway.<br>• Recommend monitoring with CloudWatch.”



---

17️⃣ Code Migration / Modernization

Field	Details

Task Description	Upgrade legacy code to modern frameworks or Python versions while preserving behavior and tests.
Steps of Execution	1️⃣ Analyze current code & dependencies.<br>2️⃣ Identify deprecated APIs.<br>3️⃣ Plan migration path.<br>4️⃣ Update syntax/features (e.g., Python 3.11).<br>5️⃣ Refactor for new libraries/frameworks.<br>6️⃣ Run regression tests.<br>7️⃣ Document changes.
Re-usability	Applicable to any tech-stack upgrade or framework migration.
Prompt A	“Migrate a Python 3.6 ETL pipeline for product reviews to Python 3.11. Include:<br>1. Identify incompatible syntax (async, f-strings).<br>2. Replace deprecated pandas functions.<br>3. Update requirements.txt with latest versions.<br>4. Add type hints throughout code.<br>5. Ensure unit tests pass unchanged.<br>6. Add GitHub Actions CI matrix for 3.9–3.11.<br>7. Provide a migration guide markdown.<br>8. Suggest performance improvements using modern features.<br>9. Maintain identical outputs.<br>10. Include profiling before/after.”
Prompt B	“Refactor a Flask sentiment API to FastAPI. Requirements:<br>• Convert routes to async endpoints.<br>• Maintain all existing functionality.<br>• Update authentication middleware.<br>• Rewrite config for Pydantic settings.<br>• Add automatic OpenAPI docs.<br>• Benchmark latency improvements.<br>• Provide docker-compose for new stack.<br>• Write migration documentation.<br>• Add compatibility tests.<br>• Suggest next steps for further modernization.”



---

18️⃣ Security Hardening & Vulnerability Scanning

Field	Details

Task Description	Audit code for security flaws and implement mitigations (injection, secrets, XSS, etc.).
Steps of Execution	1️⃣ Static analysis (Bandit, Semgrep).<br>2️⃣ Check secret leaks & hardcoded credentials.<br>3️⃣ Review API input validation.<br>4️⃣ Add rate limiting & encryption.<br>5️⃣ Implement dependency scanning.<br>6️⃣ Document all findings & fixes.
Re-usability	Essential for any production codebase, regardless of domain.
Prompt A	“Conduct a full security audit of the product-review FastAPI service. Tasks:<br>1. Scan for SQL injection points.<br>2. Verify JWT implementation.<br>3. Identify unvalidated query params.<br>4. Check CORS configuration.<br>5. Detect secrets in repo history.<br>6. Add automated Bandit checks.<br>7. Propose secure password hashing.<br>8. Recommend TLS configuration.<br>9. Produce severity-ranked report.<br>10. Provide code patches for top issues.”
Prompt B	“Create a CI pipeline step for vulnerability scanning. Include:<br>• GitHub Action using Trivy to scan Docker images.<br>• pip-audit for Python dependencies.<br>• Fail build on high-severity CVEs.<br>• Auto-generate SARIF report for GitHub Security tab.<br>• Provide sample config YAML.<br>• Send Slack notification on failures.<br>• Add badge to README.<br>• Ensure scans run nightly.<br>• Document remediation workflow.<br>• Recommend version pinning strategy.”



---

19️⃣ Logging & Observability Setup

Field	Details

Task Description	Add structured logging, metrics, and distributed tracing for debugging and monitoring.
Steps of Execution	1️⃣ Choose logging framework (loguru/structlog).<br>2️⃣ Add context-rich logs.<br>3️⃣ Integrate metrics (Prometheus/OpenTelemetry).<br>4️⃣ Add request tracing IDs.<br>5️⃣ Provide dashboards (Grafana).
Re-usability	Universal for any microservice or data pipeline.
Prompt A	“Instrument the FastAPI sentiment service. Actions:<br>1. Replace print statements with structured loguru logs.<br>2. Include trace_id and user_id in logs.<br>3. Add Prometheus metrics: request_count, latency.<br>4. Integrate OpenTelemetry tracing.<br>5. Provide Grafana dashboard JSON.<br>6. Enable log rotation and retention.<br>7. Add log-level environment variable.<br>8. Create test to ensure metrics endpoint responds.<br>9. Update deployment YAML for scraping.<br>10. Document troubleshooting guide.”
Prompt B	“Set up end-to-end observability for a Spark EDA job. Include:<br>• Structured Spark logs to CloudWatch.<br>• Metrics for executor CPU/memory.<br>• Add custom gauges for processed reviews per minute.<br>• Push logs to Elasticsearch + Kibana.<br>• Provide Terraform for all infra.<br>• Configure alert rules for job failure.<br>• Create sample Kibana dashboards.<br>• Add retention policies.<br>• Document local debug workflow.<br>• Suggest cost-optimization tips.”



---

20️⃣ Parallel & Distributed Computing Setup

Field	Details

Task Description	Convert single-threaded workflows to parallel or distributed frameworks (multiprocessing, Spark, Dask).
Steps of Execution	1️⃣ Identify CPU-heavy sections.<br>2️⃣ Choose framework.<br>3️⃣ Rewrite data processing.<br>4️⃣ Handle inter-process communication.<br>5️⃣ Add monitoring & tests.
Re-usability	Applicable to any heavy compute pipeline.
Prompt A	“Parallelize a Python script that cleans millions of product reviews. Steps:<br>1. Use multiprocessing Pool.<br>2. Ensure deterministic output order.<br>3. Add progress bar with tqdm.<br>4. Handle shared resource locks.<br>5. Benchmark speedup vs single thread.<br>6. Catch and log exceptions in workers.<br>7. Provide CLI flags for num_workers.<br>8. Add integration tests.<br>9. Profile memory usage.<br>10. Document performance results.”
Prompt B	“Port the same workflow to Apache Spark. Requirements:<br>• Read from S3 in parallel.<br>• Use mapPartitions for text cleaning.<br>• Repartition for optimal parallelism.<br>• Cache intermediate RDDs.<br>• Add unit tests using PySpark local mode.<br>• Provide Databricks job config.<br>• Monitor with Spark UI.<br>• Output to partitioned Parquet.<br>• Compare cost vs multiprocessing.<br>• Include cluster sizing guidelines.”



---

21️⃣ Data Quality & Drift Monitoring

Field	Details

Task Description	Automated checks for schema drift, anomalies, and ML model performance degradation.
Steps of Execution	1️⃣ Define quality rules.<br>2️⃣ Implement Great Expectations or custom checks.<br>3️⃣ Store baseline stats.<br>4️⃣ Set drift thresholds.<br>5️⃣ Alert via Slack/Email when violated.
Re-usability	Broadly relevant to any production data pipeline or ML model.
Prompt A	“Build a data-quality monitor for daily review feeds. Actions:<br>1. Validate schema each run.<br>2. Check null/duplicate rates.<br>3. Monitor distribution of ratings.<br>4. Compare sentiment score stats to baseline.<br>5. Trigger Slack alerts on anomalies.<br>6. Generate HTML reports.<br>7. Integrate into Airflow DAG.<br>8. Add unit tests for rules.<br>9. Provide rollback strategy.<br>10. Document how to update baselines.”
Prompt B	“Implement ML drift detection for sentiment model. Details:<br>• Track feature distributions daily.<br>• Use KS-test for significant changes.<br>• Monitor prediction confidence.<br>• Store metrics in Prometheus.<br>• Send PagerDuty alert on drift.<br>• Provide Grafana dashboard.<br>• Automate retraining trigger.<br>• Include versioned reports.<br>• Add CLI to refresh baselines.<br>• Write end-to-end tests.”



---

22️⃣ Package & Dependency Management

Field	Details

Task Description	Build maintainable Python packages with dependency pinning, virtual environments, and publishing.
Steps of Execution	1️⃣ Organize code into package structure.<br>2️⃣ Add pyproject.toml/setup.cfg.<br>3️⃣ Pin dependencies.<br>4️⃣ Add CI for build/test.<br>5️⃣ Publish to PyPI/internal index.
Re-usability	Useful for any internal or open-source Python library.
Prompt A	“Package the review-cleaning utilities as reviewkit. Steps:<br>1. Create src/reviewkit layout.<br>2. Add pyproject.toml with poetry.<br>3. Pin dependencies.<br>4. Include entry points for CLI tools.<br>5. Add README and CHANGELOG.<br>6. Write unit tests and coverage config.<br>7. Configure GitHub Actions for build & publish.<br>8. Create version bump script.<br>9. Provide install instructions.<br>10. Ensure PEP440 versioning.”
Prompt B	“Set up dependency management for a large ML repo. Actions:<br>• Use pip-tools for lockfiles.<br>• Separate dev/test/prod requirements.<br>• Add Dependabot for updates.<br>• Document upgrade procedure.<br>• Provide Makefile targets for sync.<br>• Verify reproducible builds.<br>• Add pre-commit hook to check outdated packages.<br>• Include CI test matrix for OS versions.<br>• Suggest vulnerability scanning.<br>10. Produce final dependency policy doc.”



---

23️⃣ Code Style Enforcement & Auto-Formatting

Field	Details

Task Description	Enforce consistent style and formatting with linters, formatters, and Git hooks.
Steps of Execution	1️⃣ Choose tools (black, isort, ruff).<br>2️⃣ Configure pre-commit hooks.<br>3️⃣ Add CI checks.<br>4️⃣ Document style guide.<br>5️⃣ Educate team.
Re-usability	Applies to any codebase regardless of language.
Prompt A	“Add style enforcement to product-review repo. Tasks:<br>1. Configure black and isort.<br>2. Add ruff for linting.<br>3. Set up pre-commit hooks.<br>4. Update CONTRIBUTING.md with style guide.<br>5. Fail CI on style errors.<br>6. Provide autofix instructions.<br>7. Include sample git hook config.<br>8. Add VSCode settings.json snippet.<br>9. Document upgrade process.<br>10. Include badge in README.”
Prompt B	“Introduce style checks to a multi-language project (Python + JS). Actions:<br>• Use black and eslint.<br>• Configure husky for git hooks.<br>• Add lint-staged for staged files.<br>• Provide unified CI workflow.<br>• Document common fixes.<br>• Offer script for auto-format on save.<br>• Include pre-commit sample config.<br>• Create developer onboarding guide.<br>• Monitor compliance in PR reviews.<br>• Provide metrics on style violations.”



---

24️⃣ Infrastructure as Code (IaC) Generation

Field	Details

Task Description	Automate cloud resource provisioning with Terraform/CloudFormation/Helm.
Steps of Execution	1️⃣ Identify infrastructure needs.<br>2️⃣ Model resources in IaC.<br>3️⃣ Add variables and modules.<br>4️⃣ Integrate CI/CD.<br>5️⃣ Document deployment/rollback.
Re-usability	Generic for any cloud stack (AWS/Azure/GCP).
Prompt A	“Create Terraform modules for deploying the product-review inference API. Requirements:<br>1. AWS EKS cluster.<br>2. RDS PostgreSQL instance.<br>3. S3 bucket for logs.<br>4. Parameterized VPC setup.<br>5. Output endpoint URLs.<br>6. Add remote state backend.<br>7. Implement least-privilege IAM roles.<br>8. Provide Makefile for apply/destroy.<br>9. Include automated plan in CI.<br>10. Write README with architecture diagram.”
Prompt B	“Generate Helm charts for FastAPI sentiment service. Details:<br>• Configurable replicas and resources.<br>• Environment variable injection.<br>• Rolling update strategy.<br>• Ingress configuration for HTTPS.<br>• HorizontalPodAutoscaler template.<br>• Include unit tests with helm-unittest.<br>• Provide values.yaml example.<br>• Document local testing with kind.<br>• Add CI step to lint charts.<br>• Suggest monitoring integrations.”



---

25️⃣ Interactive Visualization Dashboards

Field	Details

Task Description	Build rich dashboards for data exploration (Plotly Dash, Streamlit, React).
Steps of Execution	1️⃣ Define key metrics.<br>2️⃣ Choose visualization framework.<br>3️⃣ Build interactive charts.<br>4️⃣ Add filtering & drilldowns.<br>5️⃣ Deploy as web app.
Re-usability	Adaptable to any dataset needing stakeholder visualization.
Prompt A	“Create a Streamlit dashboard for product-review analytics. Tasks:<br>1. Show rating distribution and time trends.<br>2. Add filters for category and date range.<br>3. Display word clouds of top positive/negative words.<br>4. Provide sentiment-over-time line chart.<br>5. Add interactive product search.<br>6. Cache data for speed.<br>7. Include download buttons for CSV exports.<br>8. Support dark mode.<br>9. Containerize for deployment.<br>10. Provide README instructions.”
Prompt B	“Build a Plotly Dash app for executives. Requirements:<br>• KPI cards: avg rating, total reviews, % positive.<br>• Interactive bar chart of top products.<br>• Drill-down by region and month.<br>• Real-time refresh from database.<br>• Add authentication via company SSO.<br>• Responsive layout for mobile.<br>• Export charts to PDF.<br>• Provide Dockerfile and docker-compose.<br>• Include unit tests with dash-testing.<br>• Document deployment to Azure App Service.”



---

These 10 new tasks (16-25) match your original table format and extend your Copilot Prompt Library with detailed, reusable prompts for enterprise-grade coding assistance.




